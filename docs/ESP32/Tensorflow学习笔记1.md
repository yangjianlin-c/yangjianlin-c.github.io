


**标签**是我们要预测的事物，即$y$变量。

**特征**是输入变量，即$x$变量。

**样本**是指数据的特定实例。

样本分为以下两类：

- 有标签样本

- 无标签样本

有标签样本同时包含特征和标签。

我们使用有标签样本来训练模型。在使用有标签样本训练了我们的模型之后，我们会使用该模型来预测无标签样本的标签。

**模型**定义了特征与标签之间的关系。

**回归**模型可预测连续值。

**分类**模型可预测离散值。

**线性回归**是一种找到最适合一组点的直线或超平面的方法。

**平方误差**（又称为 **L2 误差**）

```
  = (observation - prediction(x))2
  = (y - y')2
```

**均方误差** (**MSE**) 指的是每个样本的平均平方损失。要计算 MSE，请求出各个样本的所有平方误差之和，然后除以样本数量：
$$
MSE = \frac{1}{N} \sum_{(x,y)\in D} (y - prediction(x))^2
$$
其中：

- $prediction(x)$ 指的是权重和偏差与特征集 $x$ 结合的函数。
- $D$ 指的是包含多个有标签样本（即 $(x,y)$）的数据集。


### 降低损失 (Reducing Loss)：梯度下降法

**梯度**是偏导数的矢量。梯度是一个矢量，因此具有以下两个特征：

- 方向
- 大小

梯度下降法算法会沿着负梯度的方向走一步，以便尽快降低损失。

梯度下降法算法用梯度乘以一个称为**学习速率**（有时也称为**步长**）的标量，以确定下一个点的位置。例如，如果梯度大小为 2.5，学习速率为 0.01，则梯度下降法算法会选择距离前一个点 0.025 的位置作为下一个点。

通过从我们的数据集中随机选择样本，我们可以通过小得多的数据集估算（尽管过程非常杂乱）出较大的平均值。 **随机梯度下降法** (**SGD**) 将这种想法运用到极致，它每次迭代只使用一个样本（批量大小为 1）。如果进行足够的迭代，SGD 也可以发挥作用，但过程会非常杂乱。“随机”这一术语表示构成各个批量的一个样本都是随机选择的。

**小批量随机梯度下降法**（**小批量 SGD**）是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。

## 使用 TensorFlow 的起始步骤